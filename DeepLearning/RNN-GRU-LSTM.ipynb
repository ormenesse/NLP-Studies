{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis LSTM and GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Embedding, Reshape, Activation, Input, Lambda, Dense, GRU, LSTM, CuDNNLSTM, CuDNNGRU, Dropout\n",
    "from tensorflow.python.keras.layers.merge import Dot\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import skipgrams, pad_sequences\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer # takes into consideration the morphological analysis of the words\n",
    "from nltk.stem.porter import PorterStemmer # cutting off the end or the beginning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms = punctuation + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vormenesse/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/vormenesse/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train_data\n",
      "25000 test_data\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),'train_data')\n",
    "print(len(X_test),'test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text already in tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(218, 189, 141)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('text already in tokens:')\n",
    "len(X_train[0]),len(X_train[1]),len(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "embedding_size = 10\n",
    "batch_size = 128\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre' #'post' # with you want to pad pre or post the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train, maxlen=max_len, padding=pad, truncating=pad)\n",
    "X_test_pad = pad_sequences(X_test, maxlen=max_len, padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating Model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_words,output_dim=embedding_size,input_length=max_len,\n",
    "    name='layer_embedding'                \n",
    "    ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))\n",
    "#model.add(CuDNNLSTM(128,return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid',name='classification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 256, 10)           200000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256, 10)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "classification (Dense)       (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 271,297\n",
      "Trainable params: 271,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',patience=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "186/186 [==============================] - 71s 384ms/step - loss: 0.5749 - accuracy: 0.6926 - val_loss: 0.4834 - val_accuracy: 0.7672\n",
      "Epoch 2/10\n",
      "186/186 [==============================] - 72s 388ms/step - loss: 0.3038 - accuracy: 0.8778 - val_loss: 0.3053 - val_accuracy: 0.8752\n",
      "Epoch 3/10\n",
      "186/186 [==============================] - 70s 376ms/step - loss: 0.2175 - accuracy: 0.9187 - val_loss: 0.3000 - val_accuracy: 0.8792\n",
      "Epoch 4/10\n",
      "186/186 [==============================] - 72s 389ms/step - loss: 0.1700 - accuracy: 0.9392 - val_loss: 0.4646 - val_accuracy: 0.8624\n",
      "Epoch 5/10\n",
      "186/186 [==============================] - 72s 389ms/step - loss: 0.1496 - accuracy: 0.9470 - val_loss: 0.3578 - val_accuracy: 0.8728\n",
      "Epoch 6/10\n",
      "186/186 [==============================] - 71s 384ms/step - loss: 0.1175 - accuracy: 0.9587 - val_loss: 0.5179 - val_accuracy: 0.8688\n",
      "Epoch 7/10\n",
      "186/186 [==============================] - 71s 383ms/step - loss: 0.1010 - accuracy: 0.9650 - val_loss: 0.4032 - val_accuracy: 0.8736\n",
      "Epoch 8/10\n",
      "186/186 [==============================] - 71s 384ms/step - loss: 0.0873 - accuracy: 0.9696 - val_loss: 0.4892 - val_accuracy: 0.8712\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2afb401d30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.05,\n",
    "    callbacks=[callback_early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 26s 33ms/step - loss: 0.5118 - accuracy: 0.8590\n"
     ]
    }
   ],
   "source": [
    "eval_ = model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.5117790699005127\n",
      "Accuracy 0.8590400218963623\n"
     ]
    }
   ],
   "source": [
    "print('Loss', eval_[0])\n",
    "print('Accuracy', eval_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vormenesse/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: Sentiment-LSTM/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Sentiment-LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = Sequential()\n",
    "model_GRU.add(Embedding(\n",
    "    input_dim=num_words,output_dim=embedding_size,input_length=max_len,\n",
    "    name='layer_embedding'                \n",
    "    ))\n",
    "model_GRU.add(Dropout(0.2))\n",
    "#model_GRU.add(GRU(16,dropout=0.2,recurrent_dropout=0.2))\n",
    "model_GRU.add(CuDNNGRU(units=16,return_sequences=True))\n",
    "model_GRU.add(CuDNNGRU(units=8,return_sequences=True))\n",
    "model_GRU.add(CuDNNGRU(units=4,return_sequences=False))\n",
    "model_GRU.add(Dropout(0.2))\n",
    "model_GRU.add(Dense(1,activation='sigmoid',name='classification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 256, 10)           200000    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256, 10)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_3 (CuDNNGRU)       (None, 256, 16)           1344      \n",
      "_________________________________________________________________\n",
      "cu_dnngru_4 (CuDNNGRU)       (None, 256, 8)            624       \n",
      "_________________________________________________________________\n",
      "cu_dnngru_5 (CuDNNGRU)       (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "classification (Dense)       (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 202,141\n",
      "Trainable params: 202,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',patience=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "186/186 [==============================] - 14s 75ms/step - loss: 0.5662 - accuracy: 0.7047 - val_loss: 0.4101 - val_accuracy: 0.8416\n",
      "Epoch 2/10\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 0.3595 - accuracy: 0.8602 - val_loss: 0.3547 - val_accuracy: 0.8504\n",
      "Epoch 3/10\n",
      "186/186 [==============================] - 5s 29ms/step - loss: 0.2824 - accuracy: 0.8998 - val_loss: 0.3187 - val_accuracy: 0.8696\n",
      "Epoch 4/10\n",
      "186/186 [==============================] - 5s 28ms/step - loss: 0.2437 - accuracy: 0.9144 - val_loss: 0.2904 - val_accuracy: 0.8880\n",
      "Epoch 5/10\n",
      "186/186 [==============================] - 5s 29ms/step - loss: 0.2100 - accuracy: 0.9288 - val_loss: 0.3060 - val_accuracy: 0.8824\n",
      "Epoch 6/10\n",
      "186/186 [==============================] - 5s 28ms/step - loss: 0.1840 - accuracy: 0.9403 - val_loss: 0.3376 - val_accuracy: 0.8792\n",
      "Epoch 7/10\n",
      "186/186 [==============================] - 5s 29ms/step - loss: 0.1636 - accuracy: 0.9459 - val_loss: 0.3188 - val_accuracy: 0.8792\n",
      "Epoch 8/10\n",
      "186/186 [==============================] - 5s 29ms/step - loss: 0.1494 - accuracy: 0.9512 - val_loss: 0.3192 - val_accuracy: 0.8872\n",
      "Epoch 9/10\n",
      "186/186 [==============================] - 5s 29ms/step - loss: 0.1361 - accuracy: 0.9576 - val_loss: 0.3096 - val_accuracy: 0.8816\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2b04137850>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.05,\n",
    "    callbacks=[callback_early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 13ms/step - loss: 0.3417 - accuracy: 0.8646\n"
     ]
    }
   ],
   "source": [
    "eval_ = model_GRU.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.3416987955570221\n",
      "Accuracy 0.8646399974822998\n"
     ]
    }
   ],
   "source": [
    "print('Loss', eval_[0])\n",
    "print('Accuracy', eval_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
