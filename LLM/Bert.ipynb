{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Seed for reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vormenesse\\anaconda3\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'de_core_news_sm' (3.3.0) was trained with spaCy v3.3.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\vormenesse\\anaconda3\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(680, 772)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_de.vocab.length,spacy_en.vocab.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.en') as f:\n",
    "    trainEn = f.readlines()\n",
    "with open('train.de') as f:\n",
    "    trainDe = f.readlines()\n",
    "with open('val.en') as f:\n",
    "    valEn = f.readlines()\n",
    "with open('val.de') as f:\n",
    "    valDe = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "engVocabulary = Vocabulary('en',spacy_en,40)\n",
    "deVocabulary = Vocabulary('de',spacy_de,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trainEn:\n",
    "  engVocabulary.add_sentence(i)\n",
    "for i in trainDe:\n",
    "  deVocabulary.add_sentence(i)\n",
    "for i in valEn:\n",
    "  engVocabulary.add_sentence(i)\n",
    "for i in valDe:\n",
    "  deVocabulary.add_sentence(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9951, 19062)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engVocabulary.num_words,deVocabulary.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 44)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engVocabulary.longest_sentence,deVocabulary.longest_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 461,\n",
       " 462,\n",
       " 11,\n",
       " 30,\n",
       " 169,\n",
       " 215,\n",
       " 50,\n",
       " 166,\n",
       " 209,\n",
       " 59,\n",
       " 30,\n",
       " 463,\n",
       " 464,\n",
       " 16,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deVocabulary.sentence_to_index(trainDe[100],padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 447, 241, 17, 21, 169, 155, 298, 36, 116, 373, 448, 14, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engVocabulary.sentence_to_index(trainEn[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = []\n",
    "for j in np.array(trainEn):\n",
    "    src.append(engVocabulary.sentence_to_index(j,padding=True))\n",
    "src = torch.tensor(src,dtype=torch.long, device=device)\n",
    "trg = []\n",
    "for j in np.array(trainDe):\n",
    "    trg.append(deVocabulary.sentence_to_index(j,padding=True))\n",
    "trg = torch.tensor(trg,dtype=torch.long, device=device)\n",
    "trg = trg.T\n",
    "src = src.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valsrc = []\n",
    "for j in np.array(valEn):\n",
    "    valsrc.append(engVocabulary.sentence_to_index(j,padding=True))\n",
    "valsrc = torch.tensor(valsrc,dtype=torch.long, device=device)\n",
    "valtrg = []\n",
    "for j in np.array(valDe):\n",
    "    valtrg.append(deVocabulary.sentence_to_index(j,padding=True))\n",
    "valtrg = torch.tensor(valtrg,dtype=torch.long, device=device)\n",
    "valtrg = valtrg.T\n",
    "valsrc = valsrc.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert as en encoder\n",
    "https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length = 100 #  The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    hid_dim,\n",
    "                    n_heads,\n",
    "                    pf_dim,\n",
    "                    dropout,\n",
    "                    device\n",
    "                    )\n",
    "            for _ in range(n_layers)] \n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.shape[0] #src = [batch size, src len] src_mask = [batch_size, 1, 1, src_len]\n",
    "        src_len = src.shape[1]\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device) # pos = [batch size, src len]\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)) # src = [batch size, src len, hid dim]\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask) # src = [batch_size, src len, hid dim]\n",
    "        return src\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hid_dim,\n",
    "            n_heads,\n",
    "            pf_dim, \n",
    "            dropout,\n",
    "            device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(\n",
    "            hid_dim,\n",
    "            pf_dim,\n",
    "            dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _src, attention = self.self_attention(src, src, src, src_mask) # self attention\n",
    "        src = self.self_attn_layer_norm( src + self.dropout(_src)) # dropout, residual connection and layer norm\n",
    "        # src = [batch size, src len, hid dim]\n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src)) # src = [batch size, src len, hid dim]\n",
    "        return src, attention\n",
    "    \n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hid_dim,\n",
    "            n_heads, # for parallel computing - attention is not calculated alltogether, it is calculated head by head and the concatenated\n",
    "            dropout,\n",
    "            device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads # will give an integer\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        Q = self.fc_q(query) # [batch size, query len, hid dim]\n",
    "        K = self.fc_k(key) # [batch size, key len, hid dim]\n",
    "        V = self.fc_v(value) # [batch size, value len, hid dim]\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch size, n heads, query len, hid dim]\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch size, n heads, key len, hid dim]\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch size, n heads, value len, hid dim]\n",
    "        # calculating energy - the un-normalized attention\n",
    "        energy = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale #energy = [batch size, n heads, query len, key len]\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10) # fill with zeros\n",
    "        attention = torch.softmax(energy, dim=-1) # [batch size, n heads, query len, key len] # cada linha tem que somar 1\n",
    "        x = torch.matmul(self.dropout(attention), V) #x = [batch size, n heads, query len, head dim]\n",
    "        x = x.view(batch_size, -1, self.hid_dim) # [ batch size, query len, hid dim]\n",
    "        x = self.fc_o(x) # apply layer to multi head attention [ batch size, query len, hid dim]\n",
    "        return x, attention\n",
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    #Why is this used? Unfortunately, it is never explained in the paper.\n",
    "    #BERT uses the GELU activation function, which can be used by simply switching torch.relu for F.gelu. Why did they use GELU? Again, it is never explained.\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc_1(x))) #[batch size, seq len, pf dim]\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        hid_dim, \n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        pf_dim,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length = 100 #  The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    hid_dim,\n",
    "                    n_heads,\n",
    "                    pf_dim,\n",
    "                    dropout,\n",
    "                    device\n",
    "                )\n",
    "            for _ in range(n_layers) ] \n",
    "        )\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size,1).to(self.device) # [batch size, trg len]\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos)) # [batch size, trg len, hid dim]\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # trg = [batch size, trg len, hid dim]\n",
    "            # attention = [batch size, n heads, trg len, src len]\n",
    "        output = self.fc_out(trg) # [batch size, trg len, output dim]\n",
    "        return output, attention\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hid_dim,\n",
    "        n_heads,\n",
    "        pf_dim,\n",
    "        dropout,\n",
    "        device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        # self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg)) # [ batch size, trg len, hid dim]\n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask) #attention = [batch size, n heads, trg len, src len]\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg)) # [ batch size, trg len, hid dim]\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg)) # [batch size, trg len, hid dim]\n",
    "\n",
    "        return trg, attention\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            src_pad_idx,\n",
    "            trg_pad_idx,\n",
    "            device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        #src = [batch size, src len]\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(1) # [batch size, 1, 1, src len]\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        #trg = [batch size, trg len]\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2) # [batch size, 1, 1, trg len]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool() #trg_sub_mask = [trg len, trg len] Returns the lower triangular part of the matrix\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(src) # self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src, enc_attention = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = engVocabulary.num_words+1\n",
    "OUTPUT_DIM = deVocabulary.num_words+1\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = 0\n",
    "TRG_PAD_IDX = 0\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device).to(device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device).to(device)\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(9952, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(19063, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=19063, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "            \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "# for the loss we have to ignore de PAD_TOKEN, it is not important\n",
    "_PAD_TOKEN = 0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = _PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,331,895 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def train_bert(model, optimizer, criterion, clip, src, trg):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    it = 0\n",
    "    for i in chunks(np.arange(src.shape[1]), 64):\n",
    "        it += 1\n",
    "        #print(it)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _ = model(src[:,i].to(device),trg[:,i].to(device))\n",
    "        #print('Shape')\n",
    "        #print(output.argmax(2).view(-1, output.shape[1]).shape, trg[:,i].to(device).reshape(-1,1)[:,0].shape)\n",
    "        #print(output.argmax(2).reshape(-1,1)[:,0].shape, trg[:,i].to(device).reshape(-1,1)[:,0].shape)\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), trg[:,i].to(device).reshape(-1,1)[:,0])\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    return epoch_loss / 64\n",
    "\n",
    "def evaluate_bert(model, criterion, src, trg):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for i in chunks(np.arange(src.shape[1]), 64):\n",
    "      \n",
    "      output, _ = model(src[:,i].to(device), trg[:,i].to(device)) #turn off teacher forcing\n",
    "      \n",
    "      loss = criterion(output.view(-1, output.shape[-1]), trg[:,i].to(device).reshape(-1,1)[:,0])\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "      \n",
    "      gc.collect()\n",
    "  return epoch_loss / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training 0\n",
      "Validating... 0\n",
      "Epoch: 01 | Time: 2m 25s\n",
      "\tTrain Loss: 12.554 | Train PPL: 283217.604\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Start training 1\n",
      "Validating... 1\n",
      "Epoch: 02 | Time: 2m 40s\n",
      "\tTrain Loss: 2.806 | Train PPL:  16.542\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Start training 2\n",
      "Validating... 2\n",
      "Epoch: 03 | Time: 2m 54s\n",
      "\tTrain Loss: 1.351 | Train PPL:   3.862\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Start training 3\n",
      "Validating... 3\n",
      "Epoch: 04 | Time: 3m 8s\n",
      "\tTrain Loss: 0.601 | Train PPL:   1.825\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Start training 4\n",
      "Validating... 4\n",
      "Epoch: 05 | Time: 3m 19s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Start training 5\n",
      "Validating... 5\n",
      "Epoch: 06 | Time: 3m 38s\n",
      "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Start training 6\n",
      "Validating... 6\n",
      "Epoch: 07 | Time: 3m 44s\n",
      "\tTrain Loss: 0.012 | Train PPL:   1.012\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Start training 7\n",
      "Validating... 7\n",
      "Epoch: 08 | Time: 4m 21s\n",
      "\tTrain Loss: 0.008 | Train PPL:   1.008\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Start training 8\n",
      "Validating... 8\n",
      "Epoch: 09 | Time: 4m 20s\n",
      "\tTrain Loss: 0.005 | Train PPL:   1.005\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Start training 9\n",
      "Validating... 9\n",
      "Epoch: 10 | Time: 3m 59s\n",
      "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Start training 10\n",
      "Validating... 10\n",
      "Epoch: 11 | Time: 4m 26s\n",
      "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Start training 11\n",
      "Validating... 11\n",
      "Epoch: 12 | Time: 4m 23s\n",
      "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Start training 12\n",
      "Validating... 12\n",
      "Epoch: 13 | Time: 4m 36s\n",
      "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Start training 13\n",
      "Validating... 13\n",
      "Epoch: 14 | Time: 4m 22s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Start training 14\n",
      "Validating... 14\n",
      "Epoch: 15 | Time: 4m 5s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Start training 15\n",
      "Validating... 15\n",
      "Epoch: 16 | Time: 4m 28s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Start training 16\n",
      "Validating... 16\n",
      "Epoch: 17 | Time: 6m 52s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Start training 17\n",
      "Validating... 17\n",
      "Epoch: 18 | Time: 7m 18s\n",
      "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Start training 18\n",
      "Validating... 18\n",
      "Epoch: 19 | Time: 7m 43s\n",
      "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Start training 19\n",
      "Validating... 19\n",
      "Epoch: 20 | Time: 5m 3s\n",
      "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('Start training',epoch)\n",
    "    train_loss = train_bert(model, optimizer, criterion, CLIP, src, trg)\n",
    "    print('Validating...',epoch)\n",
    "    valid_loss = evaluate_bert(model, criterion, valsrc, valtrg)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS a woman is putting a helmet on a small girl . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([engVocabulary.to_word(int(word)) for word in valsrc[:,500].cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS eine frau zieht einem kleinen mädchen einen helm an . EOS mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  output,attention_ = model(valsrc[:,500].view(-1,1).to(device),valtrg[:,500].view(-1,1).to(device))\n",
    "' '.join([deVocabulary.to_word(int(word)) for word in output.argmax(2)[:,0].cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS an artist working on an ice sculpture EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([engVocabulary.to_word(int(word)) for word in valsrc[:,600].cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS ein künstler arbeitet an einer eisskulptur EOS mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann mann'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  output,attention_ = model(valsrc[:,600].view(-1,1).to(device),valtrg[:,600].view(-1,1).to(device))\n",
    "' '.join([deVocabulary.to_word(int(word)) for word in output.argmax(2)[:,0].cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valsrc[:,600].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1, 19063])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 8, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_.squeeze(2).squeeze(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
