{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRJc9QqIKHw7"
      },
      "source": [
        "https://github.com/bentrevett/pytorch-seq2seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcnKoHkXFi1h"
      },
      "source": [
        "# Seq 2 Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jvVCupbHO8t"
      },
      "source": [
        "Tutorial Seq 2 Seq de ingles para alemão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zqoc_XsKSCs"
      },
      "source": [
        "# Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RGX566DMKXGX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functions import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rSTbs87jMSIG"
      },
      "source": [
        "\n",
        "Seed for reproduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TmOJD6KmKZ2u"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EX-cB6QdYxkk"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-qsUVkVtHkF",
        "outputId": "f9cf4e9f-dafb-469a-9a81-ee490fffd713"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U1a_abzDMz--"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vormenesse\\anaconda3\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'de_core_news_sm' (3.3.0) was trained with spaCy v3.3.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "c:\\Users\\vormenesse\\anaconda3\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtyKelqpM1TO",
        "outputId": "92d66ca2-fe02-40c0-95b3-d3544d5721bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(680, 772)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy_de.vocab.length,spacy_en.vocab.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vpdpJ7cSnshR"
      },
      "outputs": [],
      "source": [
        "with open('train.en') as f:\n",
        "    trainEn = f.readlines()\n",
        "with open('train.de') as f:\n",
        "    trainDe = f.readlines()\n",
        "with open('val.en') as f:\n",
        "    valEn = f.readlines()\n",
        "with open('val.de') as f:\n",
        "    valDe = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2Fa9EG0d6OXB"
      },
      "outputs": [],
      "source": [
        "engVocabulary = Vocabulary('en',spacy_en,40)\n",
        "deVocabulary = Vocabulary('de',spacy_de,40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QFZ0eDy_6idd"
      },
      "outputs": [],
      "source": [
        "for i in trainEn:\n",
        "  engVocabulary.add_sentence(i)\n",
        "for i in trainDe:\n",
        "  deVocabulary.add_sentence(i)\n",
        "for i in valEn:\n",
        "  engVocabulary.add_sentence(i)\n",
        "for i in valDe:\n",
        "  deVocabulary.add_sentence(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQcYvUjF6s_a",
        "outputId": "6394a0d1-aec7-4bc3-8802-5fda837ab4d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9951, 19062)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "engVocabulary.num_words,deVocabulary.num_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK0FfQsMIMvB",
        "outputId": "08d33d49-ae55-4c37-95fc-f742e91796eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(41, 44)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "engVocabulary.longest_sentence,deVocabulary.longest_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWDXikn0Ta76",
        "outputId": "5023853a-65b0-4450-c718-91617b2b7506"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 461,\n",
              " 462,\n",
              " 11,\n",
              " 30,\n",
              " 169,\n",
              " 215,\n",
              " 50,\n",
              " 166,\n",
              " 209,\n",
              " 59,\n",
              " 30,\n",
              " 463,\n",
              " 464,\n",
              " 16,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "deVocabulary.sentence_to_index(trainDe[100],padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvWC5-__8ozB",
        "outputId": "24db7d80-b00d-4ff6-8707-7e9079b41fe3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 447, 241, 17, 21, 169, 155, 298, 36, 116, 373, 448, 14, 2]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "engVocabulary.sentence_to_index(trainEn[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "src = []\n",
        "for j in np.array(trainEn):\n",
        "    src.append(engVocabulary.sentence_to_index(j,padding=True))\n",
        "src = torch.tensor(src,dtype=torch.long, device=device)\n",
        "trg = []\n",
        "for j in np.array(trainDe):\n",
        "    trg.append(deVocabulary.sentence_to_index(j,padding=True))\n",
        "trg = torch.tensor(trg,dtype=torch.long, device=device)\n",
        "trg = trg.T\n",
        "src = src.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "valsrc = []\n",
        "for j in np.array(valEn):\n",
        "    valsrc.append(engVocabulary.sentence_to_index(j,padding=True))\n",
        "valsrc = torch.tensor(valsrc,dtype=torch.long, device=device)\n",
        "valtrg = []\n",
        "for j in np.array(valDe):\n",
        "    valtrg.append(deVocabulary.sentence_to_index(j,padding=True))\n",
        "valtrg = torch.tensor(valtrg,dtype=torch.long, device=device)\n",
        "valtrg = valtrg.T\n",
        "valsrc = valsrc.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo2nILMZUr5m"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OknMtDtF8i9z"
      },
      "source": [
        "## Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q34o8bYMJrBR"
      },
      "source": [
        "https://arxiv.org/abs/1409.3215"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "U_o2FP3W8maz"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "     embedded = self.dropout(self.embedding(src))\n",
        "     outputs, (hidden, cell) = self.rnn(embedded)\n",
        "     #outputs are always from the top hidden layer\n",
        "     return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sZMsrSxeCDUt"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, cell):\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "    prediction = self.fc_out(output.squeeze(0))\n",
        "    return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XmJbeHN9S-C6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "        \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "    # should be equal because it is easier\n",
        "    assert encoder.n_layers == decoder.n_layers, \\\n",
        "        \"Encoder and decoder must have equal number of layers!\"\n",
        "  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "    batch_size = trg.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    #tensor to store decoder outputs\n",
        "    outputs_text = torch.zeros(trg_len, batch_size).to(self.device)\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    #print('outputs.shape',outputs.shape)\n",
        "    #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "    hidden, cell = self.encoder(src)\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "    outputs_text[0] = input\n",
        "    #print('input.shape',input.shape)\n",
        "    for t in range(1, trg_len):\n",
        "      #insert input token embedding, previous hidden and previous cell states\n",
        "      #receive output tensor (predictions) and new hidden and cell states\n",
        "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "      #place predictions in a tensor holding predictions for each token\n",
        "      #outputs[t] = output\n",
        "      #decide if we are going to use teacher forcing or not\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      #get the highest predicted token from our predictions\n",
        "      top1 = output.argmax(1)\n",
        "      #if teacher forcing, use actual next token as next input\n",
        "      #if not, use predicted token\n",
        "      input = trg[t] if teacher_force else top1\n",
        "      outputs[t] = output\n",
        "      outputs_text[t] = input\n",
        "      \"\"\"\n",
        "      if teacher_force:\n",
        "        zeros = torch.zeros(1,100).to(device)\n",
        "        zeros[0,input] = 1\n",
        "        outputs[t] = zeros\n",
        "      \"\"\"\n",
        "    return outputs, outputs_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1mcKROAVMv8"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30924/4079797808.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdump_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "del model\n",
        "dump_tensors()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9951, 19062)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "engVocabulary.num_words, deVocabulary.num_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7ocIlqMqVKSj"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = engVocabulary.num_words+1\n",
        "OUTPUT_DIM = deVocabulary.num_words+1\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initializing Model\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "device='cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQO1vbhGWPoD",
        "outputId": "1747d3e8-4ce7-4a1c-ea98-b7c97e49f182"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9952, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(19063, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=19063, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWbAMQDMWexC",
        "outputId": "87ef8090-ead3-4c46-eb0d-2c2bac473df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 24,563,575 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xjfVspl_WleW"
      },
      "outputs": [],
      "source": [
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "D5f98NX9WqEI"
      },
      "outputs": [],
      "source": [
        "# for the loss we have to ignore de PAD_TOKEN, it is not important\n",
        "_PAD_token = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = _PAD_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0J7Mi0OXLH8",
        "outputId": "89e6350a-bb5a-4d6b-ad3f-386e682971e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(29000, 29000)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainDe),len(trainEn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "qMXBcU4CM0hI",
        "outputId": "69cebf42-3399-4d3f-cb08-5e28c28360b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS ein älterer , übergewichtiger mann wendet einen pfannkuchen , während er frühstück macht . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src = torch.tensor(engVocabulary.sentence_to_index(valEn[100],padding=True),device=device).view(-1,1)\n",
        "#src = torch.cat((src,src))\n",
        "trg = torch.tensor(deVocabulary.sentence_to_index(valDe[100],padding=True),device=device).view(-1,1)\n",
        "#trg = torch.cat((trg,trg))\n",
        "with torch.no_grad():\n",
        "  _,output = model(src, trg, 1)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "DmpPXnvLQvqL",
        "outputId": "95d9b865-782e-4df2-c765-639e1a3aea97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS inneren lockiger lockiger bearbeitet roosevelt-fußballspieler schwarz-rot-weiße schwarz-rot-weiße geschenken geschenken bearbeitet leopard-knieschonern leopard-knieschonern straßentrommlern leopard-knieschonern leopard-knieschonern leopard-knieschonern leopard-knieschonern leopard-knieschonern leopard-knieschonern 53:11 53:11 53:11 zerstörten zerstörten grinden schwarz-rot-weiße zerstörten zerstörten zerstörten zerstörten zerstörten zerstörten zerstörten trauriges gesteinsbrocken schwarz-rot-weiße schwarz-rot-weiße schwarz-rot-weiße bearbeitet'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  _,output = model(src, trg, 0)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBFNO6fv0ujk"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0b_Y6KYYhHQ",
        "outputId": "95fe6df6-1faa-4ac5-dcd1-538b1ec083f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training 0\n",
            "Validating... 0\n",
            "Epoch: 01 | Time: 8m 19s\n",
            "\tTrain Loss: 15.922 | Train PPL: 8218121.445\n",
            "\t Val. Loss: 0.650 |  Val. PPL:   1.915\n",
            "Start training 1\n",
            "Validating... 1\n",
            "Epoch: 02 | Time: 10m 59s\n",
            "\tTrain Loss: 14.994 | Train PPL: 3249651.203\n",
            "\t Val. Loss: 0.637 |  Val. PPL:   1.891\n",
            "Start training 2\n",
            "Validating... 2\n",
            "Epoch: 03 | Time: 9m 53s\n",
            "\tTrain Loss: 14.292 | Train PPL: 1609621.855\n",
            "\t Val. Loss: 0.620 |  Val. PPL:   1.858\n",
            "Start training 3\n",
            "Validating... 3\n",
            "Epoch: 04 | Time: 10m 18s\n",
            "\tTrain Loss: 13.684 | Train PPL: 877204.688\n",
            "\t Val. Loss: 0.618 |  Val. PPL:   1.855\n",
            "Start training 4\n",
            "Validating... 4\n",
            "Epoch: 05 | Time: 9m 53s\n",
            "\tTrain Loss: 13.080 | Train PPL: 479164.955\n",
            "\t Val. Loss: 0.608 |  Val. PPL:   1.837\n",
            "Start training 5\n",
            "Validating... 5\n",
            "Epoch: 06 | Time: 9m 50s\n",
            "\tTrain Loss: 12.626 | Train PPL: 304484.745\n",
            "\t Val. Loss: 0.601 |  Val. PPL:   1.824\n",
            "Start training 6\n",
            "Validating... 6\n",
            "Epoch: 07 | Time: 10m 11s\n",
            "\tTrain Loss: 12.144 | Train PPL: 188056.733\n",
            "\t Val. Loss: 0.595 |  Val. PPL:   1.812\n",
            "Start training 7\n",
            "Validating... 7\n",
            "Epoch: 08 | Time: 10m 16s\n",
            "\tTrain Loss: 11.656 | Train PPL: 115437.169\n",
            "\t Val. Loss: 0.587 |  Val. PPL:   1.799\n",
            "Start training 8\n",
            "Validating... 8\n",
            "Epoch: 09 | Time: 10m 2s\n",
            "\tTrain Loss: 11.193 | Train PPL: 72630.292\n",
            "\t Val. Loss: 0.587 |  Val. PPL:   1.798\n",
            "Start training 9\n",
            "Validating... 9\n",
            "Epoch: 10 | Time: 9m 47s\n",
            "\tTrain Loss: 10.675 | Train PPL: 43242.240\n",
            "\t Val. Loss: 0.582 |  Val. PPL:   1.790\n",
            "Start training 10\n",
            "Validating... 10\n",
            "Epoch: 11 | Time: 10m 1s\n",
            "\tTrain Loss: 10.321 | Train PPL: 30373.597\n",
            "\t Val. Loss: 0.582 |  Val. PPL:   1.790\n",
            "Start training 11\n",
            "Validating... 11\n",
            "Epoch: 12 | Time: 10m 16s\n",
            "\tTrain Loss: 9.901 | Train PPL: 19947.987\n",
            "\t Val. Loss: 0.585 |  Val. PPL:   1.796\n",
            "Start training 12\n",
            "Validating... 12\n",
            "Epoch: 13 | Time: 9m 43s\n",
            "\tTrain Loss: 9.628 | Train PPL: 15176.996\n",
            "\t Val. Loss: 0.589 |  Val. PPL:   1.802\n",
            "Start training 13\n",
            "Validating... 13\n",
            "Epoch: 14 | Time: 9m 51s\n",
            "\tTrain Loss: 9.301 | Train PPL: 10950.339\n",
            "\t Val. Loss: 0.582 |  Val. PPL:   1.790\n",
            "Start training 14\n",
            "Validating... 14\n",
            "Epoch: 15 | Time: 10m 18s\n",
            "\tTrain Loss: 8.961 | Train PPL: 7793.257\n",
            "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
            "Start training 15\n",
            "Validating... 15\n",
            "Epoch: 16 | Time: 10m 52s\n",
            "\tTrain Loss: 8.665 | Train PPL: 5794.826\n",
            "\t Val. Loss: 0.595 |  Val. PPL:   1.813\n",
            "Start training 16\n",
            "Validating... 16\n",
            "Epoch: 17 | Time: 19m 31s\n",
            "\tTrain Loss: 8.440 | Train PPL: 4627.452\n",
            "\t Val. Loss: 0.597 |  Val. PPL:   1.817\n",
            "Start training 17\n",
            "Validating... 17\n",
            "Epoch: 18 | Time: 22m 16s\n",
            "\tTrain Loss: 8.188 | Train PPL: 3597.106\n",
            "\t Val. Loss: 0.597 |  Val. PPL:   1.816\n",
            "Start training 18\n",
            "Validating... 18\n",
            "Epoch: 19 | Time: 18m 9s\n",
            "\tTrain Loss: 7.944 | Train PPL: 2819.993\n",
            "\t Val. Loss: 0.596 |  Val. PPL:   1.814\n",
            "Start training 19\n",
            "Validating... 19\n",
            "Epoch: 20 | Time: 9m 55s\n",
            "\tTrain Loss: 7.703 | Train PPL: 2214.665\n",
            "\t Val. Loss: 0.596 |  Val. PPL:   1.815\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('Start training',epoch)\n",
        "    train_loss = train(model, optimizer, criterion, CLIP, src, trg)\n",
        "    print('Validating...',epoch)\n",
        "    valid_loss = evaluate(model, criterion, valsrc, valtrg)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40, 1015])"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valsrc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('A woman is putting a helmet on a small girl.\\n',\n",
              " 'Eine Frau zieht einem kleinen Mädchen einen Helm an.\\n')"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valEn[500],valDe[500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS eine frau hält ein baby baby , ein ein . EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  _,output = model(valsrc[:,500].view(-1,1), valtrg[:,500].view(-1,1), 0)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "k7C8YJJq7-v1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Test Loss: 0.582 | Test PPL:   1.790 |\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, criterion, valsrc, valtrg)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbTo1KOaJwCJ"
      },
      "source": [
        "# RNN Encoder-Decoder - Phrase Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRip-WbNJ3AU"
      },
      "source": [
        "https://arxiv.org/abs/1406.1078"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw95oGFAQwXx"
      },
      "source": [
        "## Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "RPmS5bWOJ2ev"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, src):\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    #outputs = [src len, batch size, hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    #outputs are always from the top hidden layer\n",
        "    return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "    self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, input, hidden, context):\n",
        "    #input = [batch size]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    #context = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "    #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "    #hidden = [1, batch size, hid dim]\n",
        "    #context = [1, batch size, hid dim]\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    emb_con = torch.cat((embedded, context), dim=2)\n",
        "    #emb_con = [1, batch size, emb dim + hid dim]\n",
        "    output, hidden = self.rnn(emb_con, hidden)\n",
        "    #output = [seq len, batch size, hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "    #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "    #output = [1, batch size, hid dim]\n",
        "    #hidden = [1, batch size, hid dim]\n",
        "    output = torch.cat((embedded.squeeze(0),hidden.squeeze(0), context.squeeze(0)),dim=1)\n",
        "    prediction = self.fc_out(output)\n",
        "    return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "      \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "\n",
        "  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "    batch_size = trg.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    #tensor to store decoder outputs\n",
        "    outputs_text = torch.zeros(trg_len, batch_size).to(self.device)\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    #print('outputs.shape',outputs.shape)\n",
        "    #last hidden state of the encoder is the context\n",
        "    context = self.encoder(src)\n",
        "    #context also used as the initial hidden state of the decoder\n",
        "    hidden = context\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "    outputs_text[0] = input\n",
        "    #print('input.shape',input.shape)\n",
        "    for t in range(1, trg_len):\n",
        "      output, hidden = self.decoder(input, hidden, context)\n",
        "      #print(output.shape)\n",
        "      # raw output with probabilities\n",
        "      outputs[t] = output\n",
        "      #decide if we are going to use teacher forcing or not\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      #get the highest predicted token from our predictions\n",
        "      top1 = output.argmax(1)\n",
        "      #if teacher forcing, use actual next token as next input\n",
        "      #if not, use predicted token\n",
        "      input = trg[t] if teacher_force else top1\n",
        "      outputs_text[t] = input\n",
        "      \"\"\"\n",
        "      if teacher_force:\n",
        "        zeros = torch.zeros(1,100).to(device)\n",
        "        zeros[0,input] = 1\n",
        "        outputs[t] = zeros\n",
        "      \"\"\"\n",
        "    return outputs, outputs_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XzlqVMZQucq"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "viwk8SZ1Q6g_"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = engVocabulary.num_words+1\n",
        "OUTPUT_DIM = deVocabulary.num_words+1\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17pvGIVvQviX",
        "outputId": "d2c11224-a6c7-464e-e107-4839a148cc6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9952, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(19063, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=19063, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT).to(device)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_BgXKu1RAQ1",
        "outputId": "a663d6e8-3bc7-42c4-8d9b-9f0d7abe2f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 34,999,415 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ThgQ96ohRDDE"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "# for the loss we have to ignore de PAD_TOKEN, it is not important\n",
        "_PAD_TOKEN = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = _PAD_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "XTYKwb7QRJ0-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training 0\n",
            "Validating... 0\n",
            "Epoch: 01 | Time: 6m 51s\n",
            "\tTrain Loss: 18.056 | Train PPL: 69424548.676\n",
            "\t Val. Loss: 0.693 |  Val. PPL:   2.000\n",
            "Start training 1\n",
            "Validating... 1\n",
            "Epoch: 02 | Time: 8m 47s\n",
            "\tTrain Loss: 15.929 | Train PPL: 8279548.015\n",
            "\t Val. Loss: 0.674 |  Val. PPL:   1.962\n",
            "Start training 2\n",
            "Validating... 2\n",
            "Epoch: 03 | Time: 10m 3s\n",
            "\tTrain Loss: 15.106 | Train PPL: 3635515.996\n",
            "\t Val. Loss: 0.670 |  Val. PPL:   1.953\n",
            "Start training 3\n",
            "Validating... 3\n",
            "Epoch: 04 | Time: 11m 32s\n",
            "\tTrain Loss: 14.385 | Train PPL: 1767653.741\n",
            "\t Val. Loss: 0.660 |  Val. PPL:   1.935\n",
            "Start training 4\n",
            "Validating... 4\n",
            "Epoch: 05 | Time: 12m 34s\n",
            "\tTrain Loss: 13.439 | Train PPL: 686073.083\n",
            "\t Val. Loss: 0.636 |  Val. PPL:   1.888\n",
            "Start training 5\n",
            "Validating... 5\n",
            "Epoch: 06 | Time: 12m 56s\n",
            "\tTrain Loss: 12.315 | Train PPL: 223012.075\n",
            "\t Val. Loss: 0.613 |  Val. PPL:   1.845\n",
            "Start training 6\n",
            "Validating... 6\n",
            "Epoch: 07 | Time: 12m 53s\n",
            "\tTrain Loss: 11.210 | Train PPL: 73856.525\n",
            "\t Val. Loss: 0.601 |  Val. PPL:   1.824\n",
            "Start training 7\n",
            "Validating... 7\n",
            "Epoch: 08 | Time: 13m 26s\n",
            "\tTrain Loss: 10.169 | Train PPL: 26083.835\n",
            "\t Val. Loss: 0.586 |  Val. PPL:   1.797\n",
            "Start training 8\n",
            "Validating... 8\n",
            "Epoch: 09 | Time: 14m 6s\n",
            "\tTrain Loss: 9.335 | Train PPL: 11327.454\n",
            "\t Val. Loss: 0.588 |  Val. PPL:   1.800\n",
            "Start training 9\n",
            "Validating... 9\n",
            "Epoch: 10 | Time: 14m 22s\n",
            "\tTrain Loss: 8.726 | Train PPL: 6162.093\n",
            "\t Val. Loss: 0.582 |  Val. PPL:   1.789\n",
            "Start training 10\n",
            "Validating... 10\n",
            "Epoch: 11 | Time: 14m 1s\n",
            "\tTrain Loss: 8.100 | Train PPL: 3294.304\n",
            "\t Val. Loss: 0.580 |  Val. PPL:   1.787\n",
            "Start training 11\n",
            "Validating... 11\n",
            "Epoch: 12 | Time: 13m 34s\n",
            "\tTrain Loss: 7.657 | Train PPL: 2115.480\n",
            "\t Val. Loss: 0.580 |  Val. PPL:   1.786\n",
            "Start training 12\n",
            "Validating... 12\n",
            "Epoch: 13 | Time: 14m 31s\n",
            "\tTrain Loss: 7.306 | Train PPL: 1488.803\n",
            "\t Val. Loss: 0.582 |  Val. PPL:   1.790\n",
            "Start training 13\n",
            "Validating... 13\n",
            "Epoch: 14 | Time: 14m 59s\n",
            "\tTrain Loss: 6.994 | Train PPL: 1090.155\n",
            "\t Val. Loss: 0.592 |  Val. PPL:   1.807\n",
            "Start training 14\n",
            "Validating... 14\n",
            "Epoch: 15 | Time: 14m 47s\n",
            "\tTrain Loss: 6.754 | Train PPL: 857.372\n",
            "\t Val. Loss: 0.595 |  Val. PPL:   1.813\n",
            "Start training 15\n",
            "Validating... 15\n",
            "Epoch: 16 | Time: 15m 34s\n",
            "\tTrain Loss: 6.531 | Train PPL: 686.301\n",
            "\t Val. Loss: 0.591 |  Val. PPL:   1.805\n",
            "Start training 16\n",
            "Validating... 16\n",
            "Epoch: 17 | Time: 15m 17s\n",
            "\tTrain Loss: 6.266 | Train PPL: 526.297\n",
            "\t Val. Loss: 0.598 |  Val. PPL:   1.818\n",
            "Start training 17\n",
            "Validating... 17\n",
            "Epoch: 18 | Time: 15m 22s\n",
            "\tTrain Loss: 6.099 | Train PPL: 445.584\n",
            "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
            "Start training 18\n",
            "Validating... 18\n",
            "Epoch: 19 | Time: 16m 25s\n",
            "\tTrain Loss: 5.906 | Train PPL: 367.067\n",
            "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
            "Start training 19\n",
            "Validating... 19\n",
            "Epoch: 20 | Time: 17m 1s\n",
            "\tTrain Loss: 5.804 | Train PPL: 331.775\n",
            "\t Val. Loss: 0.605 |  Val. PPL:   1.832\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('Start training',epoch)\n",
        "    train_loss = train(model, optimizer, criterion, CLIP, src, trg)\n",
        "    print('Validating...',epoch)\n",
        "    valid_loss = evaluate(model, criterion, valsrc, valtrg)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('A woman is putting a helmet on a small girl.\\n',\n",
              " 'Eine Frau zieht einem kleinen Mädchen einen Helm an.\\n')"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valEn[500],valDe[500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS eine frau zieht mit einem kleinen mädchen auf . EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS'"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  _,output = model(valsrc[:,500].view(-1,1), valtrg[:,500].view(-1,1), 0)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jointly Learning to Align and Translate (2015)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://arxiv.org/abs/1409.0473"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First model to use attention:\n",
        "\n",
        "Attention works by first, calculating an attention vector,  a , that is the length of the source sentence. The attention vector has the property that each element is between 0 and 1, and the entire vector sums to 1. We then calculate a weighted sum of our source sentence hidden states,  H , to get a weighted source vector,  w ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim) # * 2 because it is bidirectional\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src)) #embedded = [src len, batch size, emb dim]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        return outputs, hidden\n",
        "    \n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "    \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # transladando a matrix\n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) #energy = [batch size, src len, dec hid dim]\n",
        "        attention = self.v(energy).squeeze(2)#attention= [batch size, src len]\n",
        "        return torch.nn.functional.softmax(attention, dim=1) # returns the most important word\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        input = input.unsqueeze(0) #input = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input)) #embedded = [1, batch size, emb dim]\n",
        "        a = self.attention(hidden, encoder_outputs) # a = [batch_size, src len]\n",
        "        a = a.unsqueeze(1) #a = [batch size, 1, src len]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2) #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        #print('a',a.shape)\n",
        "        #print('encoder_outputs',encoder_outputs.shape)\n",
        "        weighted = torch.bmm(a, encoder_outputs) # batch matrix-matrix product of matrices stored in input and mat2 #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2) #weighted = [1, batch size, enc hid dim * 2]\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2) #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) #prediction = [batch size, output dim]\n",
        "        return prediction, hidden.squeeze(0)\n",
        "    \n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs_text = torch.zeros(trg_len, batch_size).to(self.device)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        outputs_text[0] = input\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            outputs_text[t] = input\n",
        "        return outputs, outputs_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Could not find module 'data' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "Error: Could not find module 'data' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "Error: Could not find module 'data' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "Error: Could not find module 'data' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "Error: Could not find module 'data' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "Error: No module named '_gdbm'\n",
            "Error: No module named '_dbm'\n",
            "Error: Module torch has no member called data\n",
            "Error: Module typing has no member called data\n",
            "Error: Unknown attribute data\n",
            "Error: No such operator quantized::data\n",
            "Error: Unknown command line flag 'ip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vormenesse\\anaconda3\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:170: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: No module named '_dbm'\n",
            "Error: No module named '_gdbm'\n",
            "Error: No module named '_dbm'\n",
            "Error: No module named '_gdbm'\n",
            "Error: Unknown command line flag 'ip'\n",
            "Tensor: GPU pinned 29000 × 40\n",
            "Tensor: GPU pinned 29000 × 40\n",
            "Tensor: GPU pinned 40 × 29000\n",
            "Tensor: GPU pinned 40 × 29000\n",
            "Tensor: GPU pinned 1015 × 40\n",
            "Tensor: GPU pinned 1015 × 40\n",
            "Tensor: GPU pinned 40 × 1015\n",
            "Tensor: GPU pinned 40 × 1015\n",
            "Total size: 4802400\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "dump_tensors(gpu_only=True)\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM = engVocabulary.num_words+1\n",
        "OUTPUT_DIM = deVocabulary.num_words+1\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9952, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(19063, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=19063, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 48,041,079 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "# for the loss we have to ignore de PAD_TOKEN, it is not important\n",
        "_PAD_TOKEN = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = _PAD_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training 0\n",
            "Validating... 0\n",
            "Epoch: 01 | Time: 39m 38s\n",
            "\tTrain Loss: 38.231 | Train PPL: 40145635791837072.000\n",
            "\t Val. Loss: 1.337 |  Val. PPL:   3.809\n",
            "Start training 1\n",
            "Validating... 1\n",
            "Epoch: 02 | Time: 31m 52s\n",
            "\tTrain Loss: 30.078 | Train PPL: 11555629576417.682\n",
            "\t Val. Loss: 1.194 |  Val. PPL:   3.300\n",
            "Start training 2\n",
            "Validating... 2\n",
            "Epoch: 03 | Time: 31m 57s\n",
            "\tTrain Loss: 24.811 | Train PPL: 59608488098.275\n",
            "\t Val. Loss: 1.134 |  Val. PPL:   3.107\n",
            "Start training 3\n",
            "Validating... 3\n",
            "Epoch: 04 | Time: 31m 35s\n",
            "\tTrain Loss: 20.595 | Train PPL: 879740350.072\n",
            "\t Val. Loss: 1.127 |  Val. PPL:   3.088\n",
            "Start training 4\n",
            "Validating... 4\n",
            "Epoch: 05 | Time: 31m 43s\n",
            "\tTrain Loss: 17.538 | Train PPL: 41354101.504\n",
            "\t Val. Loss: 1.113 |  Val. PPL:   3.043\n",
            "Start training 5\n",
            "Validating... 5\n",
            "Epoch: 06 | Time: 31m 13s\n",
            "\tTrain Loss: 15.831 | Train PPL: 7500772.932\n",
            "\t Val. Loss: 1.127 |  Val. PPL:   3.086\n",
            "Start training 6\n",
            "Validating... 6\n",
            "Epoch: 07 | Time: 30m 57s\n",
            "\tTrain Loss: 14.706 | Train PPL: 2435235.144\n",
            "\t Val. Loss: 1.147 |  Val. PPL:   3.150\n",
            "Start training 7\n",
            "Validating... 7\n",
            "Epoch: 08 | Time: 30m 35s\n",
            "\tTrain Loss: 13.607 | Train PPL: 811515.335\n",
            "\t Val. Loss: 1.146 |  Val. PPL:   3.147\n",
            "Start training 8\n",
            "Validating... 8\n",
            "Epoch: 09 | Time: 33m 2s\n",
            "\tTrain Loss: 12.754 | Train PPL: 345780.647\n",
            "\t Val. Loss: 1.160 |  Val. PPL:   3.191\n",
            "Start training 9\n",
            "Validating... 9\n",
            "Epoch: 10 | Time: 50m 28s\n",
            "\tTrain Loss: 11.946 | Train PPL: 154174.034\n",
            "\t Val. Loss: 1.182 |  Val. PPL:   3.260\n",
            "Start training 10\n",
            "Validating... 10\n",
            "Epoch: 11 | Time: 31m 3s\n",
            "\tTrain Loss: 11.338 | Train PPL: 83932.278\n",
            "\t Val. Loss: 1.207 |  Val. PPL:   3.342\n",
            "Start training 11\n",
            "Validating... 11\n",
            "Epoch: 12 | Time: 30m 43s\n",
            "\tTrain Loss: 10.761 | Train PPL: 47164.385\n",
            "\t Val. Loss: 1.241 |  Val. PPL:   3.460\n",
            "Start training 12\n",
            "Validating... 12\n",
            "Epoch: 13 | Time: 30m 27s\n",
            "\tTrain Loss: 10.278 | Train PPL: 29100.147\n",
            "\t Val. Loss: 1.233 |  Val. PPL:   3.432\n",
            "Start training 13\n",
            "Validating... 13\n",
            "Epoch: 14 | Time: 30m 35s\n",
            "\tTrain Loss: 9.804 | Train PPL: 18097.873\n",
            "\t Val. Loss: 1.270 |  Val. PPL:   3.560\n",
            "Start training 14\n",
            "Validating... 14\n",
            "Epoch: 15 | Time: 31m 59s\n",
            "\tTrain Loss: 9.539 | Train PPL: 13890.950\n",
            "\t Val. Loss: 1.274 |  Val. PPL:   3.573\n",
            "Start training 15\n",
            "Validating... 15\n",
            "Epoch: 16 | Time: 30m 38s\n",
            "\tTrain Loss: 9.211 | Train PPL: 10004.168\n",
            "\t Val. Loss: 1.294 |  Val. PPL:   3.647\n",
            "Start training 16\n",
            "Validating... 16\n",
            "Epoch: 17 | Time: 31m 24s\n",
            "\tTrain Loss: 9.075 | Train PPL: 8733.005\n",
            "\t Val. Loss: 1.318 |  Val. PPL:   3.734\n",
            "Start training 17\n",
            "Validating... 17\n",
            "Epoch: 18 | Time: 30m 11s\n",
            "\tTrain Loss: 8.860 | Train PPL: 7047.053\n",
            "\t Val. Loss: 1.325 |  Val. PPL:   3.761\n",
            "Start training 18\n",
            "Validating... 18\n",
            "Epoch: 19 | Time: 29m 48s\n",
            "\tTrain Loss: 8.661 | Train PPL: 5774.097\n",
            "\t Val. Loss: 1.358 |  Val. PPL:   3.889\n",
            "Start training 19\n",
            "Validating... 19\n",
            "Epoch: 20 | Time: 29m 33s\n",
            "\tTrain Loss: 8.486 | Train PPL: 4846.204\n",
            "\t Val. Loss: 1.361 |  Val. PPL:   3.901\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('Start training',epoch)\n",
        "    train_loss = train(model, optimizer, criterion, CLIP, src, trg)\n",
        "    print('Validating...',epoch)\n",
        "    valid_loss = evaluate(model, criterion, valsrc, valtrg)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS eine frau setzt einen kleinen kleinen an . EOS . EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  _,output = model(valsrc[:,500].view(-1,1), valtrg[:,500].view(-1,1), 0)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('A woman is putting a helmet on a small girl.\\n',\n",
              " 'Eine Frau zieht einem kleinen Mädchen einen Helm an.\\n')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valEn[500],valDe[500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS eine frau setzt einen kleinen kleinen an . EOS . EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  _,output = model(valsrc[:,500].view(-1,1), valtrg[:,500].view(-1,1), 0)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SOS junge macht kunststücke auf einem skateboard EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "SOS ein junge macht kunststücke auf einem skateboard kunststücke . EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n"
          ]
        }
      ],
      "source": [
        "del output\n",
        "with torch.no_grad():\n",
        "  _,output = model(valsrc[:,300].view(-1,1), valtrg[:,300].view(-1,1), 0)\n",
        "print(' '.join([deVocabulary.to_word(int(word)) for word in valtrg[:,300].view(-1,1).cpu().numpy()]))\n",
        "print(' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40, 1])"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Packed Padded Sequences, Masking, Inference and BLEU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook we will be adding a few improvements - packed padded sequences and masking - to the model from the previous notebook. Packed padded sequences are used to tell our RNN to skip over padding tokens in our encoder. Masking explicitly forces the model to ignore certain values, such as attention over padded elements. Both of these techniques are commonly used in NLP.\n",
        "\n",
        "We will also look at how to use our model for inference, by giving it a sentence, seeing what it translates it as and seeing where exactly it pays attention to when translating each word.\n",
        "\n",
        "Finally, we'll use the BLEU metric to measure the quality of our translations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb#scrollTo=DmF0Pm6ja079"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_len):\n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        #need to explicitly put lengths on cpu!\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'),enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        #packed_outputs is a packed sequence containing all hidden states\n",
        "        #hidden is now from the final non-padded element in the batch\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "        #outputs is now a non-packed sequence, all hidden states obtained\n",
        "        #  when the input is a pad token are all zeros\n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        return outputs, hidden\n",
        "# Attention\n",
        "# Previously, we allowed this module to \"pay attention\" to padding tokens within the source sentence. However, using masking, we can force the attention to only be over non-padding elements.\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "    \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        # repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        #attention = [batch size, src len]\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        return torch.nn.functional.softmax(attention, dim = 1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) +  emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "\n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #mask = [batch size, src len]\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        a = self.attention(hidden, encoder_outputs, mask)#a = [batch size, src len]\n",
        "        a = a.unsqueeze(1)\n",
        "        #a = [batch size, 1, src len]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        #prediction = [batch size, output dim]\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "    \n",
        "# The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference.\n",
        "# We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the forward method.\n",
        "# We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the create_mask function.\n",
        "# The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
        "# The attention at each time-step is stored in the attentions\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "    def create_mask(self,src):\n",
        "        mask = (src != self.src_pad_idx).permute(1,0)\n",
        "        return mask\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs_text = torch.zeros(trg_len, batch_size).to(self.device)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "        input = trg[0,:]\n",
        "        outputs_text[0] = input\n",
        "        #mask = [batch size, src len]\n",
        "        mask = self.create_mask(src)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            outputs_text[t] = input\n",
        "        return outputs, outputs_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'attn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28836/2915334848.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdump_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'attn' is not defined"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "del attn, enc, dec, model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "dump_tensors(gpu_only=True)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM = engVocabulary.num_words+1\n",
        "OUTPUT_DIM = deVocabulary.num_words+1\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM).to(device)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn).to(device)\n",
        "\n",
        "model = Seq2Seq(enc, dec, 0, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9952, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(19063, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=19063, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 48,041,079 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "# for the loss we have to ignore de PAD_TOKEN, it is not important\n",
        "_PAD_TOKEN = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = _PAD_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_diff(model, optimizer, criterion, clip, src, trg, src_len):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    it = 0\n",
        "    for i in chunks(np.arange(src.shape[1]), 64):\n",
        "        it += 1\n",
        "        #print(it)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, _ = model(src[:,i].to(device), src_len[i], trg[:,i].to(device))\n",
        "\n",
        "        \"\"\"\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        \"\"\"\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), trg[:,i].to(device).reshape(-1,1)[:,0])\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    return epoch_loss / 64\n",
        "\n",
        "def evaluate_diff(model, criterion, src, trg, src_len):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i in chunks(np.arange(src.shape[1]), 64):\n",
        "      \n",
        "      output, _ = model(src[:,i].to(device), src_len[i], trg[:,i].to(device),0) #turn off teacher forcing\n",
        "      \"\"\"\n",
        "      #trg = [trg len, batch size]\n",
        "      #output = [trg len, batch size, output dim]\n",
        "\n",
        "      output_dim = output.shape[-1]\n",
        "\n",
        "      output = output[1:].view(-1, output_dim)\n",
        "      trg = trg[1:].view(-1)\n",
        "\n",
        "      #trg = [(trg len ) * batch size]\n",
        "      #output = [(trg len ) * batch size, output dim]\n",
        "      \"\"\"\n",
        "      loss = criterion(output.view(-1, output.shape[-1]), trg[:,i].to(device).reshape(-1,1)[:,0])\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      \n",
        "      gc.collect()\n",
        "  return epoch_loss / 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
          ]
        }
      ],
      "source": [
        "Asrc = []\n",
        "for i in range(src.shape[1]):\n",
        "    Asrc.append(torch.sum(src[:,i] != 0))\n",
        "Asrc = torch.Tensor(Asrc,device=device)\n",
        "Vsrc = []\n",
        "for i in range(valsrc.shape[1]):\n",
        "    Vsrc.append(torch.sum(valsrc[:,i] != 0))\n",
        "Vsrc = torch.Tensor(Vsrc,device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "Asrc = []\n",
        "for i in range(src.shape[1]):\n",
        "    Asrc.append(40)\n",
        "Asrc = torch.Tensor(Asrc).to(device)\n",
        "Vsrc = []\n",
        "for i in range(valsrc.shape[1]):\n",
        "    Vsrc.append(40)\n",
        "Vsrc = torch.Tensor(Vsrc).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training 0\n",
            "Validating... 0\n",
            "Epoch: 01 | Time: 43m 19s\n",
            "\tTrain Loss: 37.326 | Train PPL: 16236398748214638.000\n",
            "\t Val. Loss: 1.252 |  Val. PPL:   3.498\n",
            "Start training 1\n",
            "Validating... 1\n",
            "Epoch: 02 | Time: 54m 27s\n",
            "\tTrain Loss: 27.952 | Train PPL: 1378132311307.431\n",
            "\t Val. Loss: 1.134 |  Val. PPL:   3.108\n",
            "Start training 2\n",
            "Validating... 2\n",
            "Epoch: 03 | Time: 55m 4s\n",
            "\tTrain Loss: 23.074 | Train PPL: 10497598496.554\n",
            "\t Val. Loss: 1.108 |  Val. PPL:   3.028\n",
            "Start training 3\n",
            "Validating... 3\n",
            "Epoch: 04 | Time: 35m 16s\n",
            "\tTrain Loss: 19.302 | Train PPL: 241410704.334\n",
            "\t Val. Loss: 1.103 |  Val. PPL:   3.013\n",
            "Start training 4\n",
            "Validating... 4\n",
            "Epoch: 05 | Time: 35m 59s\n",
            "\tTrain Loss: 16.800 | Train PPL: 19783004.235\n",
            "\t Val. Loss: 1.106 |  Val. PPL:   3.024\n",
            "Start training 5\n",
            "Validating... 5\n",
            "Epoch: 06 | Time: 27m 11s\n",
            "\tTrain Loss: 15.433 | Train PPL: 5041489.768\n",
            "\t Val. Loss: 1.120 |  Val. PPL:   3.066\n",
            "Start training 6\n",
            "Validating... 6\n",
            "Epoch: 07 | Time: 31m 32s\n",
            "\tTrain Loss: 14.383 | Train PPL: 1764001.860\n",
            "\t Val. Loss: 1.137 |  Val. PPL:   3.117\n",
            "Start training 7\n",
            "Validating... 7\n",
            "Epoch: 08 | Time: 34m 47s\n",
            "\tTrain Loss: 13.204 | Train PPL: 542549.872\n",
            "\t Val. Loss: 1.145 |  Val. PPL:   3.141\n",
            "Start training 8\n",
            "Validating... 8\n",
            "Epoch: 09 | Time: 34m 51s\n",
            "\tTrain Loss: 12.474 | Train PPL: 261407.602\n",
            "\t Val. Loss: 1.159 |  Val. PPL:   3.185\n",
            "Start training 9\n",
            "Validating... 9\n",
            "Epoch: 10 | Time: 34m 49s\n",
            "\tTrain Loss: 11.784 | Train PPL: 131150.279\n",
            "\t Val. Loss: 1.209 |  Val. PPL:   3.351\n",
            "Start training 10\n",
            "Validating... 10\n",
            "Epoch: 11 | Time: 34m 49s\n",
            "\tTrain Loss: 11.200 | Train PPL: 73123.359\n",
            "\t Val. Loss: 1.225 |  Val. PPL:   3.405\n",
            "Start training 11\n",
            "Validating... 11\n",
            "Epoch: 12 | Time: 34m 50s\n",
            "\tTrain Loss: 10.593 | Train PPL: 39865.187\n",
            "\t Val. Loss: 1.231 |  Val. PPL:   3.425\n",
            "Start training 12\n",
            "Validating... 12\n",
            "Epoch: 13 | Time: 34m 51s\n",
            "\tTrain Loss: 10.193 | Train PPL: 26708.218\n",
            "\t Val. Loss: 1.243 |  Val. PPL:   3.467\n",
            "Start training 13\n",
            "Validating... 13\n",
            "Epoch: 14 | Time: 34m 52s\n",
            "\tTrain Loss: 9.748 | Train PPL: 17124.670\n",
            "\t Val. Loss: 1.279 |  Val. PPL:   3.591\n",
            "Start training 14\n",
            "Validating... 14\n",
            "Epoch: 15 | Time: 34m 50s\n",
            "\tTrain Loss: 9.528 | Train PPL: 13735.466\n",
            "\t Val. Loss: 1.287 |  Val. PPL:   3.623\n",
            "Start training 15\n",
            "Validating... 15\n",
            "Epoch: 16 | Time: 34m 51s\n",
            "\tTrain Loss: 9.170 | Train PPL: 9605.075\n",
            "\t Val. Loss: 1.306 |  Val. PPL:   3.691\n",
            "Start training 16\n",
            "Validating... 16\n",
            "Epoch: 17 | Time: 34m 52s\n",
            "\tTrain Loss: 9.002 | Train PPL: 8117.678\n",
            "\t Val. Loss: 1.335 |  Val. PPL:   3.802\n",
            "Start training 17\n",
            "Validating... 17\n",
            "Epoch: 18 | Time: 34m 34s\n",
            "\tTrain Loss: 8.746 | Train PPL: 6286.449\n",
            "\t Val. Loss: 1.359 |  Val. PPL:   3.892\n",
            "Start training 18\n",
            "Validating... 18\n",
            "Epoch: 19 | Time: 34m 51s\n",
            "\tTrain Loss: 8.609 | Train PPL: 5480.065\n",
            "\t Val. Loss: 1.342 |  Val. PPL:   3.825\n",
            "Start training 19\n",
            "Validating... 19\n",
            "Epoch: 20 | Time: 34m 46s\n",
            "\tTrain Loss: 8.412 | Train PPL: 4500.228\n",
            "\t Val. Loss: 1.369 |  Val. PPL:   3.933\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('Start training',epoch)\n",
        "    train_loss = train_diff(model, optimizer, criterion, CLIP, src, trg, Asrc)\n",
        "    print('Validating...',epoch)\n",
        "    valid_loss = evaluate_diff(model, criterion, valsrc, valtrg, Vsrc)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([40.], device='cuda:0')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Asrc[1:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('A woman is putting a helmet on a small girl.\\n',\n",
              " 'Eine Frau zieht einem kleinen Mädchen einen Helm an.\\n')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valEn[500],valDe[500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS eine frau zieht einen helm auf einem kleinen mädchen . EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  _,output = model(valsrc[:,500].view(-1,1).to(device), Asrc[1:2],valtrg[:,500].view(-1,1).to(device), 0)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention is all you need - BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb#scrollTo=IfU7dDZ8a06O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook we will be implementing a (slightly modified version) of the Transformer model from the Attention is All You Need paper (https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762). All images in this notebook will be taken from the Transformer paper. For more information about the Transformer, see these three articles:\n",
        "- https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.mihaileric.com%2Fposts%2Ftransformers-attention-in-disguise%2F\n",
        "- https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fjalammar.github.io%2Fillustrated-transformer%2F\n",
        "- https://colab.research.google.com/corgiredirector?site=http%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction\n",
        "Similar to the Convolutional Sequence-to-Sequence model, the Transformer does not use any recurrence. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, attention mechanisms and normalization.\n",
        "\n",
        "As of January 2020, Transformers are the dominant architecture in NLP and are used to achieve state-of-the-art results for many tasks and it appears as if they will be for the near future.\n",
        "\n",
        "The most popular Transformer variant is BERT (Bidirectional Encoder Representations from Transformers) https://arxiv.org/pdf/1810.04805.pdf and pre-trained versions of BERT are commonly used to replace the embedding layers - if not more - in NLP models.\n",
        "\n",
        "A common library used when dealing with pre-trained transformers is the Transformers library (https://huggingface.co/docs/transformers/index), see here for a list of all pre-trained models available.\n",
        "\n",
        "The differences between the implementation in this notebook and the paper are:\n",
        "\n",
        "we use a learned positional encoding instead of a static one\n",
        "we use the standard Adam optimizer with a static learning rate instead of one with warm-up and cool-down steps\n",
        "we do not use label smoothing\n",
        "We make all of these changes as they closely follow BERT's set-up and the majority of Transformer variants use a similar set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 hid_dim,\n",
        "                 n_layers,\n",
        "                 n_heads,\n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device,\n",
        "                 max_length = 100 #  The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    hid_dim,\n",
        "                    n_heads,\n",
        "                    pf_dim,\n",
        "                    dropout,\n",
        "                    device\n",
        "                    )\n",
        "            for _ in range(n_layers)] \n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        batch_size = src.shape[0] #src = [batch size, src len] src_mask = [batch_size, 1, 1, src_len]\n",
        "        src_len = src.shape[1]\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device) # pos = [batch size, src len]\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)) # src = [batch size, src len, hid dim]\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask) # src = [batch_size, src len, hid dim]\n",
        "        return src\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            hid_dim,\n",
        "            n_heads,\n",
        "            pf_dim, \n",
        "            dropout,\n",
        "            device\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(\n",
        "            hid_dim,\n",
        "            pf_dim,\n",
        "            dropout\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        _src, _ = self.self_attention(src, src, src, src_mask) # self attention\n",
        "        src = self.self_attn_layer_norm( src + self.dropout(_src)) # dropout, residual connection and layer norm\n",
        "        # src = [batch size, src len, hid dim]\n",
        "        # positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src)) # src = [batch size, src len, hid dim]\n",
        "        return src\n",
        "    \n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            hid_dim,\n",
        "            n_heads, # for parallel computing - attention is not calculated alltogether, it is calculated head by head and the concatenated\n",
        "            dropout,\n",
        "            device\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert hid_dim % n_heads == 0\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads # will give an integer\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "    \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "        Q = self.fc_q(query) # [batch size, query len, hid dim]\n",
        "        K = self.fc_k(key) # [batch size, key len, hid dim]\n",
        "        V = self.fc_v(value) # [batch size, value len, hid dim]\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch size, n heads, query len, hid dim]\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch size, n heads, key len, hid dim]\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch size, n heads, value len, hid dim]\n",
        "        # calculating energy - the un-normalized attention\n",
        "        energy = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale #energy = [batch size, n heads, query len, key len]\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10) # fill with zeros\n",
        "        attention = torch.softmax(energy, dim=-1) # [batch size, n heads, query len, key len] # cada linha tem que somar 1\n",
        "        x = torch.matmul(self.dropout(attention), V) #x = [batch size, n heads, query len, head dim]\n",
        "        x = x.view(batch_size, -1, self.hid_dim) # [ batch size, query len, hid dim]\n",
        "        x = self.fc_o(x) # apply layer to multi head attention [ batch size, query len, hid dim]\n",
        "        return x, attention\n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    #Why is this used? Unfortunately, it is never explained in the paper.\n",
        "    #BERT uses the GELU activation function, which can be used by simply switching torch.relu for F.gelu. Why did they use GELU? Again, it is never explained.\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x))) #[batch size, seq len, pf dim]\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "# Decoder\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim,\n",
        "        hid_dim, \n",
        "        n_layers,\n",
        "        n_heads,\n",
        "        pf_dim,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length = 100 #  The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderLayer(\n",
        "                    hid_dim,\n",
        "                    n_heads,\n",
        "                    pf_dim,\n",
        "                    dropout,\n",
        "                    device\n",
        "                )\n",
        "            for _ in range(n_layers) ] \n",
        "        )\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size,1).to(self.device) # [batch size, trg len]\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos)) # [batch size, trg len, hid dim]\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "            # trg = [batch size, trg len, hid dim]\n",
        "            # attention = [batch size, n heads, trg len, src len]\n",
        "        output = self.fc_out(trg) # [batch size, trg len, output dim]\n",
        "        return output, attention\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hid_dim,\n",
        "        n_heads,\n",
        "        pf_dim,\n",
        "        dropout,\n",
        "        device\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        # self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg)) # [ batch size, trg len, hid dim]\n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask) #attention = [batch size, n heads, trg len, src len]\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg)) # [ batch size, trg len, hid dim]\n",
        "        # positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        # dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg)) # [batch size, trg len, hid dim]\n",
        "\n",
        "        return trg, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seq2Seq\n",
        "\n",
        "Finally, we have the `Seq2Seq` module which encapsulates the encoder and decoder, as well as handling the creation of the masks.\n",
        "\n",
        "The source mask is created by checking where the source sequence is not equal to a `<pad>` token. It is 1 where the token is not a `<pad>` token and 0 when it is. It is then unsqueezed so it can be correctly broadcast when applying the mask to the `energy`, which of shape **_[batch size, n heads, seq len, seq len]_**.\n",
        "\n",
        "The target mask is slightly more complicated. First, we create a mask for the `<pad>` tokens, as we did for the source mask. Next, we create a \"subsequent\" mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal will be zero and the elements below the diagonal will be set to whatever the input tensor is. In this case, the input tensor will be a tensor filled with ones. So this means our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
        "\n",
        "$$\\begin{matrix}\n",
        "1 & 0 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 1\\\\\n",
        "\\end{matrix}$$\n",
        "\n",
        "This shows what each target token (row) is allowed to look at (column). The first target token has a mask of **_[1, 0, 0, 0, 0]_** which means it can only look at the first target token. The second target token has a mask of **_[1, 1, 0, 0, 0]_** which it means it can look at both the first and second target tokens. \n",
        "\n",
        "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
        "\n",
        "$$\\begin{matrix}\n",
        "1 & 0 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}$$\n",
        "\n",
        "After the masks are created, they used with the encoder and decoder along with the source and target sentences to get our predicted target sentence, `output`, along with the decoder's attention over the source sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            encoder,\n",
        "            decoder,\n",
        "            src_pad_idx,\n",
        "            trg_pad_idx,\n",
        "            device\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        #src = [batch size, src len]\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(1) # [batch size, 1, 1, src len]\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        #trg = [batch size, trg len]\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2) # [batch size, 1, 1, trg len]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool() #trg_sub_mask = [trg len, trg len] Returns the lower triangular part of the matrix\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        return trg_mask\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'enc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26328/1113203276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdump_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'enc' is not defined"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "del enc, dec, model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "dump_tensors(gpu_only=True)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM = engVocabulary.num_words+1\n",
        "OUTPUT_DIM = deVocabulary.num_words+1\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "SRC_PAD_IDX = 0\n",
        "TRG_PAD_IDX = 0\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device).to(device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device).to(device)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(9952, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(19063, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=19063, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "            \n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "# for the loss we have to ignore de PAD_TOKEN, it is not important\n",
        "_PAD_TOKEN = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = _PAD_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 16,331,895 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "def train_bert(model, optimizer, criterion, clip, src, trg):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    it = 0\n",
        "    for i in chunks(np.arange(src.shape[1]), 64):\n",
        "        it += 1\n",
        "        #print(it)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, _ = model(src[:,i].to(device),trg[:,i].to(device))\n",
        "        #print('Shape')\n",
        "        #print(output.argmax(2).view(-1, output.shape[1]).shape, trg[:,i].to(device).reshape(-1,1)[:,0].shape)\n",
        "        #print(output.argmax(2).reshape(-1,1)[:,0].shape, trg[:,i].to(device).reshape(-1,1)[:,0].shape)\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), trg[:,i].to(device).reshape(-1,1)[:,0])\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    return epoch_loss / 64\n",
        "\n",
        "def evaluate_bert(model, criterion, src, trg):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i in chunks(np.arange(src.shape[1]), 64):\n",
        "      \n",
        "      output, _ = model(src[:,i].to(device), trg[:,i].to(device)) #turn off teacher forcing\n",
        "      \n",
        "      loss = criterion(output.view(-1, output.shape[-1]), trg[:,i].to(device).reshape(-1,1)[:,0])\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      \n",
        "      gc.collect()\n",
        "  return epoch_loss / 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'schiri faden entlangblickt karten gefüttert beton schaufensterfront faden empfangsportier ohrhörer erlernt akustischen metallkonstruktur orangeflammender kerlen befingert lümmelt schaufensterfront schaufensterfront schaffneruniform laternen straßenecke gesäßtasche ausstellungsbereich gassigehen 14 erhältlich teil rohes hemdbluse flussbetts haarfestiger steuerung zusammensein regale laternen entreißen raping kinderfahrrad boote'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  output,attention_ = model(valsrc[:,500].view(-1,1).to(device),valtrg[:,500].view(-1,1).to(device))\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output.argmax(2)[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training 0\n",
            "Validating... 0\n",
            "Epoch: 01 | Time: 3m 53s\n",
            "\tTrain Loss: 9.357 | Train PPL: 11584.140\n",
            "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
            "Start training 1\n",
            "Validating... 1\n",
            "Epoch: 02 | Time: 5m 13s\n",
            "\tTrain Loss: 2.707 | Train PPL:  14.978\n",
            "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
            "Start training 2\n",
            "Validating... 2\n",
            "Epoch: 03 | Time: 4m 13s\n",
            "\tTrain Loss: 1.313 | Train PPL:   3.717\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Start training 3\n",
            "Validating... 3\n",
            "Epoch: 04 | Time: 4m 54s\n",
            "\tTrain Loss: 0.578 | Train PPL:   1.782\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Start training 4\n",
            "Validating... 4\n",
            "Epoch: 05 | Time: 4m 59s\n",
            "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Start training 5\n",
            "Validating... 5\n",
            "Epoch: 06 | Time: 3m 41s\n",
            "\tTrain Loss: 0.023 | Train PPL:   1.023\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Start training 6\n",
            "Validating... 6\n",
            "Epoch: 07 | Time: 6m 40s\n",
            "\tTrain Loss: 0.011 | Train PPL:   1.011\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Start training 7\n",
            "Validating... 7\n",
            "Epoch: 08 | Time: 7m 47s\n",
            "\tTrain Loss: 0.008 | Train PPL:   1.008\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Start training 8\n",
            "Validating... 8\n",
            "Epoch: 09 | Time: 3m 38s\n",
            "\tTrain Loss: 0.005 | Train PPL:   1.005\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Start training 9\n",
            "Validating... 9\n",
            "Epoch: 10 | Time: 4m 19s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Start training 10\n",
            "Validating... 10\n",
            "Epoch: 11 | Time: 3m 42s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Start training 11\n",
            "Validating... 11\n",
            "Epoch: 12 | Time: 3m 38s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
            "Start training 12\n",
            "Validating... 12\n",
            "Epoch: 13 | Time: 3m 29s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Start training 13\n",
            "Validating... 13\n",
            "Epoch: 14 | Time: 3m 23s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Start training 14\n",
            "Validating... 14\n",
            "Epoch: 15 | Time: 3m 23s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Start training 15\n",
            "Validating... 15\n",
            "Epoch: 16 | Time: 3m 21s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Start training 16\n",
            "Validating... 16\n",
            "Epoch: 17 | Time: 3m 19s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Start training 17\n",
            "Validating... 17\n",
            "Epoch: 18 | Time: 3m 24s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Start training 18\n",
            "Validating... 18\n",
            "Epoch: 19 | Time: 3m 27s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Start training 19\n",
            "Validating... 19\n",
            "Epoch: 20 | Time: 3m 28s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('Start training',epoch)\n",
        "    train_loss = train_bert(model, optimizer, criterion, CLIP, src, trg)\n",
        "    print('Validating...',epoch)\n",
        "    valid_loss = evaluate_bert(model, criterion, valsrc, valtrg)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "SRC_PAD_IDX = 0\n",
        "TRG_PAD_IDX = 0\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              'cpu').to('cpu')\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              'cpu').to('cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, 'cpu').to('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut5-model.pt','cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS eine frau zieht einem kleinen mädchen einen helm an . EOS . . . . . . . . . . . . . . . . . . . . . . . . . . . .'"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  output,attention_ = model(valsrc[:,500].view(-1,1).to(device),valtrg[:,500].view(-1,1).to(device))\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output.argmax(2)[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SOS a woman is putting a helmet on a small girl . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD'"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join([engVocabulary.to_word(int(word)) for word in valsrc[:,500].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = [engVocabulary.to_word(int(word)) for word in valsrc[:,500].cpu().numpy()]\n",
        "translation = [deVocabulary.to_word(int(word)) for word in output.argmax(2)[:,0].cpu().numpy()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference - Translation\n",
        "\n",
        "Now we can can translations from our model with the `translate_sentence` function below.\n",
        "\n",
        "The steps taken are:\n",
        "- tokenize the source sentence if it has not been tokenized (is a string)\n",
        "- append the `<sos>` and `<eos>` tokens\n",
        "- numericalize the source sentence\n",
        "- convert it to a tensor and add a batch dimension\n",
        "- create the source sentence mask\n",
        "- feed the source sentence and mask into the encoder\n",
        "- create a list to hold the output sentence, initialized with an `<sos>` token\n",
        "- while we have not hit a maximum length\n",
        "  - convert the current output sentence prediction into a tensor with a batch dimension\n",
        "  - create a target sentence mask\n",
        "  - place the current output, encoder output and both masks into the decoder\n",
        "  - get next output token prediction from decoder along with attention\n",
        "  - add prediction to current output sentence prediction\n",
        "  - break if the prediction was an `<eos>` token\n",
        "- convert the output sentence from indexes to tokens\n",
        "- return the output sentence (with the `<sos>` token removed) and the attention from the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# translating Sentence\n",
        "\n",
        "PAD_token = 0   # Used for padding short sentences\n",
        "SOS_token = 1   # Start-of-sentence token\n",
        "EOS_token = 2   # End-of-sentence token\n",
        "\n",
        "max_len = 40\n",
        "\n",
        "model.eval()\n",
        "\n",
        "src_tensor = valsrc[:,500].view(-1,1).to(device)\n",
        "\n",
        "src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "with torch.no_grad():\n",
        "    enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "trg_indexes = [SOS_token]*max_len\n",
        "\n",
        "for i in range(0):\n",
        "\n",
        "    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device).view(-1,1)\n",
        "\n",
        "    trg_mask = model.make_trg_mask(trg_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "    \n",
        "    pred_token = output.argmax(2)[-1,:].item()\n",
        "    \n",
        "    trg_indexes[i] = pred_token\n",
        "\n",
        "    if pred_token == EOS_token:\n",
        "        break\n",
        "\n",
        "trg_tokens = [deVocabulary.to_word(int(word)) for word in output.argmax(2)[:,0].cpu().numpy()]\n",
        "\n",
        "#return trg_tokens[1:], attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trg_indexes = [10]*max_len\n",
        "trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device).view(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  output,attention_ = model(valsrc[:,500].view(-1,1).to(device),trg_tensor)\n",
        "' '.join([deVocabulary.to_word(int(word)) for word in output.argmax(2)[:,0].cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward(self, src, trg):\n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Displaying Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\vormenesse\\AppData\\Local\\Temp/ipykernel_26328/541027858.py:21: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(['']+sentence, rotation=45)\n",
            "C:\\Users\\vormenesse\\AppData\\Local\\Temp/ipykernel_26328/541027858.py:22: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(['']+translation)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x1800 with 8 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "n_heads = 8\n",
        "n_rows = 4\n",
        "n_cols = 2\n",
        "\n",
        "assert n_rows * n_cols == n_heads\n",
        "\n",
        "fig = plt.figure(figsize=(15,25))\n",
        "\n",
        "for i in range(n_heads):\n",
        "    \n",
        "    ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "    \n",
        "    _attention = attention_.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "    cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_xticklabels(['']+sentence, rotation=45)\n",
        "    ax.set_yticklabels(['']+translation)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
